<!DOCTYPE html>
<html>
    <head>
        <meta charset="utf-8">
        <title>Learning Machine Learning</title>
        <!-- Including the font from google fonts -->
        <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,300italic,300,600,400italic,600italic,700,700italic,800,800italic' rel='stylesheet' type='text/css'>
        <!-- Including the libraries -->
        <script src="/libs/d3/d3.min.js"></script>
        <!-- libraries for the map -->
        <script src="/libs/d3.geo.projection.min.js"></script>
        <script src="/libs/topojson.js"></script>

        <!-- including the JS to make math look nice -->
        <script type="text/javascript" async
          src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
        </script>
        <!-- including script for code snippets -->
        <script src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js?lang=python&amp;skin=desert"></script>
        <!-- Including the CSS -->
        <link rel="stylesheet" href="/css/arcadetrail.css" media="screen" charset="utf-8">
        <link rel="stylesheet" href="/css/d3.geomap.css" media="screen" charset="utf-8">
    </head>

    <body>
            <h1> Learning Machine Learning </h1>
            <p>
              In this blog post we will implement a machine learning algorithm
              to recommend games to users based on the game they like owe and like.
              The data is from
              <a href="https://arcadetrail.com/upcoming"> www.arcadetrail.com </a> — check it
              out, its awesome.
            </p>
            <h2> Introduction </h2>
            <p>
              First a bit about my background. I’m currently a Ph.d.
              student in experimental physics and I have a master’s degree in
              mathematics and physics from the University of Roskilde. While I use
              <a href="http://se.mathworks.com/index.html?s_tid=gn_logo"> MatLab</a>
              every day for controlling experiments, analysing data and
              making pretty figures, I have had no formal training in computer science or
              any programming languages. All I know about MatLab I've learned though using the “help X”
              function in MatLab or through Googling “How to X in MatLab”. Over
              the last three years I have become quite efficient in MatLab, but
              I often run into problems where my fundamental knowledge about
              programming languages become dauntingly obvious.
            </p>
            <p>
              So before starting this Machine Learning project I had a firm understanding
              of mathematics and experience with numerical programming.
            </p>
            <p>
              In the beginning of January 2016 I decided to follow
              <a href="https://www.coursera.org/learn/machine-learning/">Coursera's
              Machine Learning course taught</a> by
              <a href="http://www.andrewng.org/"> Andrew Ng</a>.
            </p>
              This blogpost will be divided into three parts. In part one I will
              go through the algorithm behind the reccomender system we will be
              building. If you are allready familiar with the collaborative filtering
              method you may wish to skim this part quickly. In the second part
              I will show how I built the reccomender system in Python. This was
              my first encounter with Python, so anyone new to Python may find
              it interesting. Anyone experience experience in Python my well cringe
              at ineffectiveness of my coding. In the last part I will reflect on
              what I've learned building this thing in Pythons focusing on the
              challanges I think other in similar positins will face.
            </p>
            <h2> A walkthough of the collaberative filtering method -
              The main ideas and the mathematics </h2>
            <p>
              Technically, the type of recommender system we will implement is
              called collaborative filtering. We will construct at Matrix with the
              size \( (# users)x(# games) \). This matrix
              contains all the votes for each user. At the time of writing this
              the Arcade trail database had 350 users and 4601 games, so \( R \)
              was a 350 by 4601 matrix. The matrix
              is incomplete in the sense that not every user will have voted for
              every game. The aim is to establish which games a user might like,
              that is, we will fill out the missing parts of the \(R\) matrix. We
              do this by finding the matrixes \( X \) and \( \Theta \) whose
              <a href="https://en.wikipedia.org/wiki/Matrix_multiplication"> (matrix)
                product </a> gives the filled out matrix \( R^* \):
              $$ \Theta X^T = R^* $$
              Here \( \Theta \) and \( X \)  are \(# users\) by 1 and \(# games\)
              by 1 matrices respectively. They have to be, otherwise the
              product would give \( R^* \) the wrong size.
            </p>
            <p>
              In principle this is all there is to it.
            </p>
            <p>
              To get the matrices \( X \) and \( \Theta \) we could simply try out
              numbers untill we hit ones with product that is reasonably close to
              \( R \). But this would be a tedious task even for quite small matrices.
              What we want is for the computer to do all
              the heavy lifting. For this we must come up with an mathematical expression
              that the computer can minimize. This is expression is usually called the cost
              function, presumably because it is the penalty the coomputer must
              pay for putting up a wrong answer. The cost function we will use
              is:
              $$ J(X,\Theta, R) =
              \frac{1}{2} \sum \left( \Theta X^T  - R \right)^{2} +
              \frac{\lambda}{2}\sum \Theta^{2}
              + \frac{\lambda}{2}\sum X^{2}
              $$
              Here \( A^2 \) is shorthand for squaring every entry in \( A \).
              That is \(A^2 = A \circ A \) and not \( A \cdot A \) with \( \circ \)
              being is the symbol for the
              <a href="https://en.wikipedia.org/wiki/Hadamard_product_(matrices)">
              entrywise product</a> and \( \cdot \) and the
              <a href="https://en.wikipedia.org/wiki/Dot_product">dot product</a>
              respectively.
            </p>
            <p>
              Lets look closer at what the cost function actually does. The first
              term is the squared difference between the prediction votes
              and the acutal votes. If the the difference is high the cost is high.
              The next two term are regularization terms. They are there to increase
              the cost of the using high values for the parameters. \( \lambda \)
              is a parameter we can chose freely, depending on how much we wish to
              supress high parameter values. If you use a very low value of \( \lambda
              \) chances are your optimization code will take ages to run and
              produce terribly results. So in the end the cost function produces a
              single number. If the cost is high, our computer did a bad job of
              reproducing \( R \).
            </p>
            <p>
              Since we are dealing with \( # user + # games \) parameters the
              optimizaing code need some extra help to run smoothly.
            </p>
            <p>
              The gradient of \( X \) is given by:
              $$ X_{grad} =
              \left( \Theta X^T - R \right)\cdot \Theta
              + \lambda X $$
              While the gradient of \( \Theta \) is given by:
              $$ \Theta_{grad} =
              \left( \Theta X^T - R \right)\cdot X
              + \lambda \Theta $$
              Again, here \( \circ \) is the symbol for the
              <a href="https://en.wikipedia.org/wiki/Hadamard_product_(matrices)">
              entrywise product</a> and \( \cdot \) is the
              <a href="https://en.wikipedia.org/wiki/Dot_product">dot product</a>.
              Since the dot product produces a number (a scalar) \( X_{grad} \)
              and \( \Theta_{grad} \) have the same size as \( X \) and \( \Theta \)
              respectively. As, of course, they should.
            </p>
            <p>
              Supplying the optimization code with the cost function and the
              gradient it will find the best values for \( X \) and \( \Theta \).
            </p>
            <p>
               To get the reccomendation for a specific user we simply need to
               do the following calculation:
              $$ r = \Theta_\text{user} X^T. $$
              This produces a matrix of size 1 by \( # games \) that include the
              probability (a number between 0 and 1) that the user would vote for
              each game. Getting the top ten reccomendations  ammounts
              to picking out the top ten values from \( r \).
            </p>
            <p>
              At the time of writing this, the reccomendation I get is:
              <ol>
                  <li>Mirror's Edge Catalyst</li>
                  <li>Portal</li>
                  <li>Cities Skylines: After Dark</li>
                  <li>Portal 2</li>
                  <li>Left 4 Dead 2</li>
                  <li>Counter-Strike: Source</li>
                  <li>Half-Life 2: Lost Coast</li>
                  <li>Half-Life 2</li>
                  <li>Uncharted 4 - A Thiefs End</li>
                  <li>Counter-Strike</li>
              </ol>
              Which is not half bad.
            </p>
            <h2>The Python implementation</h2>
            <p>
              Here I will go through my Python implementation of the collaberative
              filtering method.
            </p>
            <p>
              First I import all the packages I use.
            </p>
<pre class="prettyprint">
from numpy import *
import numpy as np
import operator
from os import listdir
import csv
import itertools
</pre>
          <p>
            Then I load the data. I have not included the code here since parsing
            functions are tedious to read and write. So lets move on to what happens
            after the data has been parsed into structures that Python understands.
            I construct the \( R \) matrix from the information we have about
            the users of Arcadetrail.com. We know which games they have voted
            on and which games they owe. I have chosen to count both votes and
            ownerships equally, and its doesn't matter if you have both voted for
            and owe a game -- it will only count once.
          </p>
<pre class="prettyprint">
def construct_R(votes,ownerships,num_users,num_games):
	R = zeros((num_users,num_games))
	for ii in range(0,len(votes)):
	    current_user = int(votes[ii,1])-1 #-1 because python starts at 0
	    current_game = int(votes[ii,0])-1
	    #print(current_user,current_game)
	    R[current_user,current_game] = 1
	# Filling up with user ownerships
	for ii in range(0,len(ownerships)):
	    current_user = int(ownerships[ii,1])-1
	    current_game = int(ownerships[ii,0])-1
	    R[current_user,current_game] = 1
	return R
</pre>
        <p>
          The code for the cost function is quite long -- this is the main reason
          why i prefer to write down all the equations on paper first. It is all
          too easy to loose track of the terms when writing the Python code.
        </p>
<pre class="prettyprint">
def cost_function(params, Y, R, num_users, num_games,num_features, regularization):
	X = np.reshape(params[0:num_games*num_features], (num_games, num_features))
	Theta = np.reshape(params[num_games*num_features:], (num_users, num_features))

	# calculating the prediction
	prediction = Theta * np.transpose(X)
	difference = ( prediction - Y )
	square_difference = np.multiply(difference,difference)
	square_difference_votes = np.multiply(square_difference,R)

	theta_squared = np.multiply(Theta,Theta)
	theta_regularized = (regularization/2)*sum(theta_squared)

	X_squared = np.multiply(X,X)
	X_regularized = (regularization/2)*sum(X_squared)

	cost = 0.5* sum(square_difference_votes) + theta_regularized + X_regularized
	return cost
</pre>
          <p>
            Calculating the gradient of the cost function.
          </p>
<pre class="prettyprint">
def cost_grad(params, Y, R, num_users, num_games,num_features, regularization):
  X = np.reshape(params[0:num_games*num_features], (num_games, num_features))
  Theta = np.reshape(params[num_games*num_features:], (num_users, num_features))

  # calculating X_grad
  prediction = Theta*np.transpose(X)
  difference = prediction - Y
  voted = np.multiply(difference,R)
  X_grad = np.dot(np.transpose(voted),Theta) + regularization*X

  # calculating Theta_grad
  Theta_grad = np.dot(voted,X) + regularization*Theta

  # rolling paramaters out
  grad = np.append(X_grad,Theta_grad)
  return(grad)
</pre>
          <p>
            The code for getting the \( \Theta \) and \( X \) matrices is mainly running the
            fmin_cg function from scipy library. The rest of the code is just reshaping the
            \( \Theta \) and \( X \) matrices into a single vector to simplify the optimization
            and then putting them back to their natural size when then optimization is done.
          </p>
<pre class="prettyprint">
  def collaberative_filtering(num_games,num_users,num_features,R,regularization):
  	import scipy.optimize
  	import ReccomenderSystem
  	X = np.random.randint(2,size=(num_games,num_features))
  	Theta = np.random.randint(2,size=(num_users,num_features))

  	X_long = np.reshape(X,(size(X),1))
  	Theta_long = np.reshape(Theta,size(Theta),1)
  	initial_parameters = np.append(X_long,Theta_long)

  	Y = R

  	# doing the actual fitting
  	params = scipy.optimize.fmin_cg(
  	    f = ReccomenderSystem.cost_function,
  	    x0 = initial_parameters,
  	    fprime = ReccomenderSystem.cost_grad,
  	    args=(Y, R, num_users, num_games,num_features, regularization),
  	    maxiter=200)

  	# Rolling the fitted parameters back into X and Theta
  	X = np.reshape(params[0:num_games*num_features], (num_games, num_features))
  	Theta = np.reshape(params[num_games*num_features:], (num_users, num_features))
  	return X, Theta
</pre>
          <p>
            Now we have all we need to reccomend games to user! The following function gets
            the reccomendations for a user specified by the user id.
          </p>
<pre class = "prettyprint">
def getting_reccomendations(R,Theta,X,titles,user_names,user_id):
	user_index = user_id-1 # Fixing the indexing
	prediction = Theta * np.transpose(X)
	reccomendations = prediction[user_index,:]
	# removing from the list of reccomandations games the user allready likes
	to_remove = np.where(R[user_index,:]==1)
	reccomendations[to_remove] = 0

	# sorting the list of reccomendations
	sortIndex = sorted(range(len(reccomendations)), key=lambda k: reccomendations[k],reverse=True)

	# finding the user name
	user_names = np.matrix(user_names)
	name_index = np.where(user_names[:,0]==str(user_id))
	name_index = int(name_index[0])
	name = user_names[name_index,1]

	titles = np.matrix(titles)

	# printing the top 10 reccomendations
	print 'The top 10 reccomendations for %s' % name + ' (user_id %s' % str(user_index+1) +')'
	for i in range(0,10,1):#range(10):
	    title_index = np.where(titles[:,0]==str(sortIndex[i]+1))
	    print(i+1, titles[title_index[0][0],1])
</pre>
          <p>
            This concludes the Python implementation of the collaberative filtering
            reccomender system.
          </p>
          <h2>What remains to be done </h2>
          <p>
            Building this reccomender system was fun. However, there are a number
            of obvious improvements. For example, I know my older brother is not
            a big fan of first person shooters. Yet, on the list of reccomendations
            for him I find: Counter-Strike, Counter-Strike: Source,
            Counter-Strike: Condition Zero, Counter-Strike: Global Offensive and
            Half-Life 2: Deathmatch. Clearly the reccomender system would really
            like for him to try out Counter Strike.
          </p>
          <P>
            Luckily there is such a thing as a  content based reccomender
            system! This is what I will be playing with next.
          </p>
    </body>
</html>
